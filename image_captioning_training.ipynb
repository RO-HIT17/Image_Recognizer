{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images : 31764\n",
      "Image: 1000092795.jpg\n",
      "Captions: ['Two young guys with shaggy hair look at their hands while hanging out in the yard .', 'Two young  White males are outside near many bushes .', 'Two men in green shirts are standing in a yard .', 'A man in a blue shirt standing in a garden .', 'Two friends enjoy time spent together .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "json_file_path = 'C:\\\\Rohit\\\\Projects\\\\Image Data Set\\\\Flickr30k\\\\annotations.json'\n",
    "image_directory = 'C:\\\\Rohit\\\\Projects\\\\Image Data Set\\\\Flickr30k\\\\images'\n",
    "\n",
    "\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total Images : {len(data.items())}\")\n",
    "\n",
    "for image_name, captions in data.items():\n",
    "    print(f\"Image: {image_name}\")\n",
    "    print(f\"Captions: {captions['comments']}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(299, 299)):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img) / 255.0 \n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all captions from JSON\n",
    "all_captions = []\n",
    "for captions in data.values():\n",
    "    all_captions.extend(captions['comments'])\n",
    "\n",
    "# Tokenize captions\n",
    "tokenizer = Tokenizer(oov_token='<UNK>', lower=True)\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert captions to sequences\n",
    "captions_sequences = {}\n",
    "for image_name, captions in data.items():\n",
    "    captions_sequences[image_name] = tokenizer.texts_to_sequences(captions['comments'])\n",
    "\n",
    "# Define max length of captions\n",
    "max_caption_length = 100  # Adjust this based on your data\n",
    "\n",
    "# Pad sequences to ensure all captions have the same length\n",
    "for image_name in captions_sequences:\n",
    "    captions_sequences[image_name] = pad_sequences(captions_sequences[image_name], maxlen=max_caption_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of image file names\n",
    "image_files = list(data.keys())\n",
    "\n",
    "# Split image file names into training and validation sets\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training images: {len(train_images)}, Validation images: {len(val_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(image_files, batch_size=32):\n",
    "    while True:\n",
    "        for i in range(0, len(image_files), batch_size):\n",
    "            batch_images = image_files[i:i + batch_size]\n",
    "            images, captions = [], []\n",
    "            for image_name in batch_images:\n",
    "                # Preprocess the image\n",
    "                image_path = os.path.join(image_directory, image_name)\n",
    "                image = preprocess_image(image_path)\n",
    "                images.append(image)\n",
    "                \n",
    "                # Select a random caption for each image\n",
    "                caption_seq = captions_sequences[image_name]\n",
    "                random_caption = caption_seq[np.random.randint(0, len(caption_seq))]\n",
    "                captions.append(random_caption)\n",
    "                \n",
    "            yield np.array(images), np.array(captions)\n",
    "\n",
    "# Example of generating a batch\n",
    "train_generator = data_generator(train_images, batch_size=4)\n",
    "train_batch = next(train_generator)\n",
    "print(f\"Image batch shape: {train_batch[0].shape}, Caption batch shape: {train_batch[1].shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
