{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images : 31764\n",
      "Image: 1000092795.jpg\n",
      "Captions: ['Two young guys with shaggy hair look at their hands while hanging out in the yard .', 'Two young  White males are outside near many bushes .', 'Two men in green shirts are standing in a yard .', 'A man in a blue shirt standing in a garden .', 'Two friends enjoy time spent together .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "json_file_path = 'C:\\\\Rohit\\\\Projects\\\\Image Data Set\\\\Flickr30k\\\\annotations.json'\n",
    "image_directory = 'C:\\\\Rohit\\\\Projects\\\\Image Data Set\\\\Flickr30k\\\\images'\n",
    "\n",
    "\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total Images : {len(data.items())}\")\n",
    "\n",
    "for image_name, captions in data.items():\n",
    "    print(f\"Image: {image_name}\")\n",
    "    print(f\"Captions: {captions['comments']}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(299, 299)):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img) / 255.0 \n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_captions = []\n",
    "for captions in data.values():\n",
    "    all_captions.extend(captions['comments'])\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<UNK>', lower=True)\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "captions_sequences = {}\n",
    "for image_name, captions in data.items():\n",
    "    captions_sequences[image_name] = tokenizer.texts_to_sequences(captions['comments'])\n",
    "\n",
    "\n",
    "max_caption_length = 20  # Initially was set as 20 changed to 100\n",
    "\n",
    "for image_name in captions_sequences:\n",
    "    captions_sequences[image_name] = pad_sequences(captions_sequences[image_name], maxlen=max_caption_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 25411, Validation images: 6353\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_files = list(data.keys())\n",
    "\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training images: {len(train_images)}, Validation images: {len(val_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def data_generator(image_files, captions_sequences, image_directory, batch_size=32, max_caption_length=20):\n",
    "    while True:\n",
    "        for i in range(0, len(image_files), batch_size):\n",
    "            batch_images = image_files[i:i + batch_size]\n",
    "            images, captions_input, captions_target = [], [], []\n",
    "            \n",
    "            for image_name in batch_images:\n",
    "                # Load and preprocess the image\n",
    "                image_path = os.path.join(image_directory, image_name)\n",
    "                image = preprocess_image(image_path)  # Your image preprocessing function\n",
    "                images.append(image)\n",
    "                \n",
    "                # Randomly select a caption for this image\n",
    "                caption_seq = captions_sequences[image_name]  # List of possible captions for the image\n",
    "                random_caption = caption_seq[np.random.randint(0, len(caption_seq))]  # Random caption\n",
    "                \n",
    "                # Prepare input (caption excluding the last word) and target (caption shifted by one word)\n",
    "                caption_input = random_caption[:-1]  # Input to the model (all words except the last one)\n",
    "                caption_target = random_caption[1:]  # Target output (shifted by one word)\n",
    "\n",
    "                # Append the input and target captions\n",
    "                captions_input.append(caption_input)\n",
    "                captions_target.append(caption_target)\n",
    "            \n",
    "            # Pad captions to ensure consistent sequence length to max_caption_length\n",
    "            captions_input = pad_sequences(captions_input, maxlen=max_caption_length, padding='post')\n",
    "            captions_target = pad_sequences(captions_target, maxlen=max_caption_length, padding='post')\n",
    "\n",
    "            # Convert to categorical if needed\n",
    "            captions_target = to_categorical(captions_target, num_classes=vocab_size)\n",
    "\n",
    "            # Convert to numpy arrays and yield the batch\n",
    "            yield (np.array(images), np.array(captions_input)), np.array(captions_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(image_files, captions_sequences, image_directory, batch_size=32, max_caption_length=20):\n",
    "    # Define the output signature for the dataset\n",
    "    output_signature = (\n",
    "        (tf.TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32),  # Images\n",
    "         tf.TensorSpec(shape=(None, max_caption_length), dtype=tf.int32)),  # Input Captions\n",
    "        tf.TensorSpec(shape=(None, max_caption_length), dtype=tf.int32)  # Target Captions\n",
    "    )\n",
    "\n",
    "    # Wrap the generator using tf.data.Dataset.from_generator\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(image_files, captions_sequences, image_directory, batch_size, max_caption_length),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[4691200,18317] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:StatelessRandomUniformV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m max_caption_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# Set this to the maximum length of your captions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Create the model\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_caption_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Display the model summary\u001b[39;00m\n\u001b[0;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[17], line 28\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(vocab_size, embedding_dim, max_caption_length)\u001b[0m\n\u001b[0;32m     25\u001b[0m combined \u001b[38;5;241m=\u001b[39m Concatenate()([image_features, lstm_out])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Dense layer for output (one output per time step)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Create the model\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[image_input, caption_input], outputs\u001b[38;5;241m=\u001b[39moutput)\n",
      "File \u001b[1;32mc:\\Rohit\\Projects\\Image Describer\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Rohit\\Projects\\Image Describer\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\random.py:34\u001b[0m, in \u001b[0;36muniform\u001b[1;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[0;32m     32\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mor\u001b[39;00m floatx()\n\u001b[0;32m     33\u001b[0m seed \u001b[38;5;241m=\u001b[39m _cast_seed(draw_seed(seed))\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[4691200,18317] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:StatelessRandomUniformV2] name: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten, Concatenate\n",
    "from tensorflow.keras.applications import InceptionV3  # Example CNN\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the create_model function\n",
    "def create_model(vocab_size, embedding_dim, max_caption_length):\n",
    "    # Input for image features (using a CNN model like InceptionV3)\n",
    "    image_input = Input(shape=(299, 299, 3))  # Example shape for preprocessed image\n",
    "    cnn_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')  # Example CNN\n",
    "    image_features = cnn_model(image_input)  # Extract features from the image\n",
    "\n",
    "    # Input for captions\n",
    "    caption_input = Input(shape=(max_caption_length,))  # Shape of input captions\n",
    "    \n",
    "    # Embedding layer for captions\n",
    "    caption_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(caption_input)\n",
    "    \n",
    "    # LSTM layer for caption generation\n",
    "    lstm_out = LSTM(256, return_sequences=True)(caption_embedding)\n",
    "    \n",
    "    # Concatenate image features with LSTM output\n",
    "    lstm_out = Flatten()(lstm_out)  # Flatten LSTM output to match image features shape\n",
    "    combined = Concatenate()([image_features, lstm_out])\n",
    "    \n",
    "    # Dense layer for output (one output per time step)\n",
    "    output = Dense(vocab_size, activation='softmax')(combined)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, caption_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy')  # Change to sparse_categorical_crossentropy\n",
    "    return model\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 18317  # Adjust this based on your actual vocabulary size\n",
    "embedding_dim = 256  # You can choose the embedding size as needed\n",
    "max_caption_length = 20  # Set this to the maximum length of your captions\n",
    "\n",
    "# Create the model\n",
    "model = create_model(vocab_size, embedding_dim, max_caption_length)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Rohit\\Projects\\Image Describer\\env\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_624']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 20), output.shape=(None, 18317)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m create_tf_dataset(val_images, captions_sequences, image_directory, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Rohit\\Projects\\Image Describer\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Rohit\\Projects\\Image Describer\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:652\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m     )\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m--> 652\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    654\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    656\u001b[0m     )\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 20), output.shape=(None, 18317)"
     ]
    }
   ],
   "source": [
    "# Set batch size and number of epochs\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "# Create the TensorFlow datasets for training and validation\n",
    "train_dataset = create_tf_dataset(train_images, captions_sequences, image_directory, batch_size=batch_size)\n",
    "val_dataset = create_tf_dataset(val_images, captions_sequences, image_directory, batch_size=batch_size)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('image_captioning_model.h5')\n",
    "print(\"Model Created Succesfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
